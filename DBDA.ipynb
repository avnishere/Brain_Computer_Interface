{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DBDA.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["OpP5Hh5wqsia","EAqZNQ7Tqsib","KWL7LTR9qsic","c0nSlbjiqsid","yuy6Fqfnqsid","9wT0eWLNqsii","Yd1HjKTsqsii","lSKvohg5L3KK"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OpP5Hh5wqsia","colab_type":"text"},"source":["# Dataset information\n","ref: http://bbci.de/competition/iii/desc_V.html"]},{"cell_type":"markdown","metadata":{"id":"EAqZNQ7Tqsib","colab_type":"text"},"source":["## Experiment\n","\n","This dataset contains data from 3 normal subjects during 4 non-feedback sessions. The subjects sat in a normal chair, relaxed arms resting on their legs. There are 3 tasks:\n","1. Imagination of repetitive self-paced left hand movements, (_left_, class 2),\n","2. Imagination of repetitive self-paced right hand movements, (_right_, class 3),\n","3. Generation of words beginning with the same random letter, (_word_, class 7).\n","\n","All 4 sessions of a given subject were acquired on the same day, each lasting 4 minutes with 5-10 minutes breaks in between them. The subject performed a given task for about 15 seconds and then switched randomly to another task at the operator's request. EEG data is not splitted in trials since the subjects are continuously performing any of the mental tasks. The algorithm should provide an output every 0.5 seconds using the last second of data (see clarification in the paragraph 'Requirements and Evaluation'.) Data are provided in two ways:\n","1. _Raw EEG_ signals. Sampling rate was 512 Hz.\n","2. _Precomputed_ features. The raw EEG potentials were first spatially filtered by means of a surface Laplacian. Then, every 62.5 ms --i.e., 16 times per second-- the power spectral density (PSD) in the band 8-30 Hz was estimated over the last second of data with a frequency resolution of 2 Hz for the 8 centro-parietal channels C3, Cz, C4, CP1, CP2, P3, Pz, and P4. As a result, an EEG sample is a 96-dimensional vector (8 channels times 12 frequency components)."]},{"cell_type":"markdown","metadata":{"id":"KWL7LTR9qsic","colab_type":"text"},"source":["## Format of the Data\n","\n","For each subject there are 3 training files and 1 testing file (the last recording session). Training files are labelled while testing files are not. Data are provided in ASCII format.\n","- _Precomputed features_: files contain a PSD sample per row (i.e., the first 12 components are the PSD in the band 8-30 Hz at channel C3, and so on, for a total of 96 components). The number of PSD samples are:\n","\n","| &nbsp; | **Training** | **Testing** |\n","| --- | :---: | :---: |\n","| Subject 1 | 3488/3472/3568 | 3504 |\n","| Subject 2 | 3472/3456/3472 | 3472 |\n","| Subject 3 | 3424/3424/3440 | 3488 |\n","\n","- _Raw EEG signals_: each line of the files contains the 32 EEG potentials acquired at a given time instant in the order: Fp1, AF3, F7, F3, FC1, FC5, T7, C3, CP1, CP5, P7, P3, Pz, PO3, O1, Oz, O2, PO4, P4, P8, CP6, CP2, C4, T8, FC6, FC2, F4, F8, AF4, Fp2, Fz, Cz. In the training files, each line has a 33rd component indicating the class label."]},{"cell_type":"markdown","metadata":{"id":"c0nSlbjiqsid","colab_type":"text"},"source":["## Requirements and Evaluation\n","\n","Please provide your estimated class labels (2, 3, or 7) for every input vector of the 3 test files (one per subject). The labels must be estimated in the following way:\n","\n","- **Precomputed features:** Since input vectors are computed 16 times per second, provide the average of 8 consecutive samples (so that to get a response every 0.5 seconds). Other (i.e. also past) samples must not be used in order to guarantee a fast response times of the system, although for the competition test data set averaging over more samples could be of benefit.\n","\n","- **Raw signals:** Compute vectors 16 times per second using the last second of data. Then provide the average of 8 consecutive samples (so that to get a response every 0.5 seconds). Other (i.e. also past) samples must not be used in order to guarantee a fast response times of the system, although for the competition test data set averaging over more samples could be of benefit.\n","\n","Also give a description of the used algorithm. The performance measure is the classification accuracy (correct classification divided by the total number of samples) averaged over the 3 subjects. \n","There will be a special prize to the best algorithm working with the precomputed samples (in the case it does not achieve the best absolute result)."]},{"cell_type":"markdown","metadata":{"id":"yuy6Fqfnqsid","colab_type":"text"},"source":["## Technical Information\n","\n","EEG signals were recorded with a Biosemi system using a cap with 32 integrated electrodes located at standard positions of the International 10-20 system. The sampling rate was 512 Hz. Signals were acquired at full DC. No artifact rejection or correction was employed."]},{"cell_type":"markdown","metadata":{"id":"ejLG1MVfnsSU","colab_type":"text"},"source":["Quick summary:\n","\n","*   Continuous, multi-class dataset.\n","*   8 channels (electrode positions) * 12 frequencies (8:2:30 Hz) = 96 dimensional vector\n","*   Small data size"]},{"cell_type":"markdown","metadata":{"id":"zqM9EDIVqsie","colab_type":"text"},"source":["---\n","# Main Tasks"]},{"cell_type":"code","metadata":{"id":"6Je6OUkqqsif","colab_type":"code","colab":{}},"source":["import os\n","from scipy.io import loadmat\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# import seaborn as sns\n","%matplotlib inline\n","\n","import pickle\n","from collections import Counter\n","import re\n","\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9wT0eWLNqsii","colab_type":"text"},"source":["## Read Data"]},{"cell_type":"markdown","metadata":{"id":"VKUn9bgNj7Gl","colab_type":"text"},"source":["### Mount google drive - Skip this if running locally"]},{"cell_type":"code","metadata":{"id":"buThuWRhjOvM","colab_type":"code","colab":{}},"source":["# Mount Google drive\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdkRlyuMjqoh","colab_type":"code","colab":{}},"source":["# Here you must try by yourself where the path to the dataset folder is !!\n","# e.g. !ls \"gdrive/My Drive/Colab Notebooks/DME/datasets\"\n","!ls \"gdrive\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YSu2jDSckK5B","colab_type":"code","colab":{}},"source":["# Then assign the path here.\n","# This is the path for my GG drive, change it according to your path\n","# dataset_path = \"gdrive/My Drive/Colab Notebooks/DME/datasets\"\n","\n","# Your folder path here\n","dataset_path = \"gdrive/My Drive/Colab Notebooks/DME/datasets\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FqSsDNBpqsip","colab_type":"text"},"source":["### Read data from csv"]},{"cell_type":"code","metadata":{"id":"PV64-5xDqsip","colab_type":"code","colab":{}},"source":["try:\n","  dataset_path\n","except:\n","  dataset_path = os.path.join(os.getcwd(), 'datasets')\n","\n","df_train_pcf = pd.read_csv(os.path.join(dataset_path, 'df_train_pcf.csv'))\n","df_test_pcf = pd.read_csv(os.path.join(dataset_path, 'df_test_pcf.csv'))\n","df_train_raw = pd.read_csv(os.path.join(dataset_path, 'df_train_raw.csv'))\n","df_test_raw = pd.read_csv(os.path.join(dataset_path, 'df_test_raw.csv'))\n","\n","label_dict = {2: 'left', 3: 'right', 7: 'word'}\n","df_train_pcf.replace({'label' : label_dict}, inplace=True)\n","df_train_raw.replace({'label' : label_dict}, inplace=True)\n","\n","pcf_setup = pickle.load(open(os.path.join(dataset_path, 'pcf_setup.obj'), \"rb\"))\n","raw_setup = pickle.load(open(os.path.join(dataset_path, 'raw_setup.obj'), \"rb\"))\n","\n","pcf_channels = pcf_setup['channels']\n","raw_channels = raw_setup['channels']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yd1HjKTsqsii","colab_type":"text"},"source":["### !!! Skip this step !!! if no needs of MAT files !!!\n","### Read data from seperated mat files"]},{"cell_type":"code","metadata":{"id":"bG1zRYC3qsij","colab_type":"code","colab":{}},"source":["def load_datasets(filenames, folder=os.path.join(os.getcwd(), 'datasets'), mapping_fn=None, test=False):\n","    list_df = []\n","    for file in filenames:\n","        path = os.path.join(folder, file)\n","        mat = loadmat(path)\n","        list_df.append(mat_to_df(mat, mapping_fn=mapping_fn, ignore_label=test))\n","    return pd.concat(list_df, ignore_index=True)\n","\n","def psd_mapping(mat):\n","    # Return X, Y, filename, channels, n_components\n","    return {\n","        'X': mat.get('X'),\n","        'Y': mat.get('Y'),\n","        'filename': mat['nfo']['name'][0][0][0],\n","        'channels': [c for [c] in mat['nfo']['clab'][0][0][0]],\n","        'n_components': 12,\n","        'fs': mat['nfo']['fs'][0][0][0][0],\n","        'xpos': mat['nfo']['xpos'][0][0].reshape(-1),\n","        'ypos': mat['nfo']['ypos'][0][0].reshape(-1)\n","    }\n","\n","def raw_eeg_mapping(mat):\n","    # Return X, Y, filename, channels, n_components\n","    return {\n","        'X': mat.get('X'),\n","        'Y': mat.get('Y'),\n","        'filename': mat['nfo']['name'][0][0][0],\n","        'channels': [c for [c] in mat['nfo']['clab'][0][0][0]],\n","        'n_components': 1,\n","        'fs': mat['nfo']['fs'][0][0][0][0],\n","        'xpos': mat['nfo']['xpos'][0][0].reshape(-1),\n","        'ypos': mat['nfo']['ypos'][0][0].reshape(-1)\n","    }\n","\n","def mat_to_df(mat, mapping_fn=None, ignore_label=False):\n","\n","    assert callable(mapping_fn), \"Missing mapping function (mapping_fn).\\nInput: mat\\nOutput: X, Y, filename, channels, n_components\"\n","\n","    data = mapping_fn(mat)\n","    X = data['X']\n","    Y = data['Y']\n","    filename = data['filename']\n","    channels = data['channels']\n","    n_components = data['n_components']\n","\n","    pattern = '^(?:train|test)_subject(\\d+)_\\w+?(\\d+)$'\n","    match = re.search(pattern, filename)\n","\n","    assert match is not None, \"Filename does not match regex '{}'\".format(pattern)\n","\n","    subject = int(match.group(1))\n","    session = int(match.group(2))\n","\n","    n_rows, n_cols = X.shape\n","\n","    if n_components > 1:\n","        columns = [\"{}_{}\".format(c,i+1) for c in channels for i in range(n_components) ] + ['subject', 'session']\n","    else:\n","        columns = channels + ['subject', 'session']\n","    dtypes = [np.float64] * n_cols + [np.uint8] * 3\n","    mixed_data = np.hstack((\n","        X,\n","        subject * np.ones((n_rows, 1), dtype=int),\n","        session * np.ones((n_rows, 1), dtype=int),\n","    ))\n","\n","    if ignore_label == False:\n","        assert Y is not None, \"Missing label data.\\nSolution: mat_to_df(mat, ignore_label=True)\"\n","        columns += ['label']\n","        dtypes += [np.uint8]\n","        mixed_data = np.hstack((mixed_data, Y))\n","\n","    df = pd.DataFrame(mixed_data, columns=columns)\n","    df = df.astype(dict(zip(columns, dtypes)))\n","    return df\n","\n","def extract_channels(filename, folder=os.path.join(os.getcwd(), 'datasets'), mapping_fn=None):\n","    path = os.path.join(folder, filename)\n","    mat = loadmat(path)\n","    data = mapping_fn(mat)\n","    return data['channels']\n","\n","def extract_setup(filename, folder=os.path.join(os.getcwd(), 'datasets'), mapping_fn=None):\n","    path = os.path.join(folder, filename)\n","    mat = loadmat(path)\n","    setup = ['channels', 'fs', 'xpos', 'ypos']\n","    return {k: v for k, v in mapping_fn(mat).items() if k in setup}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qTXc0nxqqsil","colab_type":"code","colab":{}},"source":["train_precomputed_features_data_files = [\n","    'train_subject1_psd01.mat',\n","    'train_subject1_psd02.mat',\n","    'train_subject1_psd03.mat',\n","    'train_subject2_psd01.mat',\n","    'train_subject2_psd02.mat',\n","    'train_subject2_psd03.mat',\n","    'train_subject3_psd01.mat',\n","    'train_subject3_psd02.mat',\n","    'train_subject3_psd03.mat'\n","]\n","test_precomputed_features_data_files = [\n","    'test_subject1_psd04.mat',\n","    'test_subject2_psd04.mat',\n","    'test_subject3_psd04.mat'\n","]\n","train_raw_eeg_signals_data_files = [\n","    'train_subject1_raw01.mat',\n","    'train_subject1_raw02.mat',\n","    'train_subject1_raw03.mat',\n","    'train_subject2_raw01.mat',\n","    'train_subject2_raw02.mat',\n","    'train_subject2_raw03.mat',\n","    'train_subject3_raw01.mat',\n","    'train_subject3_raw02.mat',\n","    'train_subject3_raw03.mat'\n","]\n","test_raw_eeg_signals_data_files = [\n","    'test_subject1_raw04.mat',\n","    'test_subject2_raw04.mat',\n","    'test_subject3_raw04.mat'\n","]\n","\n","df_train_pcf = load_datasets(train_precomputed_features_data_files, mapping_fn=psd_mapping)\n","df_test_pcf = load_datasets(test_precomputed_features_data_files, mapping_fn=psd_mapping, test=True)\n","df_train_raw = load_datasets(train_raw_eeg_signals_data_files, mapping_fn=raw_eeg_mapping)\n","df_test_raw = load_datasets(test_raw_eeg_signals_data_files, mapping_fn=raw_eeg_mapping, test=True)\n","\n","pcf_setup = extract_setup(train_precomputed_features_data_files[0], mapping_fn=psd_mapping)\n","raw_setup = extract_setup(train_raw_eeg_signals_data_files[0], mapping_fn=raw_eeg_mapping)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FSxm4ndRqsin","colab_type":"code","colab":{}},"source":["# Save data as csv\n","df_train_pcf.to_csv(os.path.join(os.getcwd(), 'datasets', 'df_train_pcf.csv'), index=False)\n","df_test_pcf.to_csv(os.path.join(os.getcwd(), 'datasets', 'df_test_pcf.csv'), index=False)\n","df_train_raw.to_csv(os.path.join(os.getcwd(), 'datasets', 'df_train_raw.csv'), index=False)\n","df_test_raw.to_csv(os.path.join(os.getcwd(), 'datasets', 'df_test_raw.csv'), index=False)\n","\n","import pickle\n","with open(os.path.join(os.getcwd(), 'datasets', 'pcf_setup.obj'), \"wb\") as f:\n","    pickle.dump(pcf_setup, f, pickle.HIGHEST_PROTOCOL)\n","with open(os.path.join(os.getcwd(), 'datasets', 'raw_setup.obj'), \"wb\") as f:\n","    pickle.dump(raw_setup, f, pickle.HIGHEST_PROTOCOL)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a9IVa6fOit9o","colab_type":"text"},"source":["## DBDA\n","ref: https://publications.idiap.ch/downloads/papers/2008/Galan_THESIS_2008.pdf"]},{"cell_type":"code","metadata":{"id":"H57CbqNViwpi","colab_type":"code","colab":{}},"source":["def df_channel_norm(df, channels):\n","    # output: [0,1]\n","    data = df.copy()\n","    for channel in pcf_channels:\n","        df_channel = data.filter(regex=channel)\n","        data[df_channel.columns] = df_channel.div(df_channel.sum(axis=1), axis='index')\n","    return data\n","\n","def df_channel_standardize(df, channels):\n","    # output: zero mean, unit variance\n","    data = df.copy()\n","    for channel in pcf_channels:\n","        df_channel = data.filter(regex=channel)\n","        data[df_channel.columns] = df_channel.sub(df_channel.mean(axis=1), axis='index').div(df_channel.std(axis=1), axis='index')\n","    return data\n","\n","def cvt(df):\n","    data = df.copy()\n","    data_columns = data.drop(columns=['label', 'session', 'subject']).columns\n","\n","    n = len(data)\n","    n_l = data['label'].value_counts()\n","    labels = n_l.keys()\n","\n","    m = data[data_columns].mean()\n","    m_l = { label: data[data['label'] == label][data_columns].mean() for label in labels }\n","\n","    d = len(data_columns) # number of feature\n","    B = np.zeros((d, d))\n","    W = np.zeros((d, d))\n","    for label in labels:\n","        diff = np.array(m_l[label] - m).reshape((d, 1))\n","        B += n_l[label] * diff @ diff.T\n","        for i, psd in data[data_columns].iterrows():\n","            diff = np.array(psd - m_l[label]).reshape((d, 1))\n","            W += diff @ diff.T\n","\n","    E, A = np.linalg.eigh(np.linalg.inv(W) @ B)\n","    eigen_vec = A[:, E > 0]\n","    Y = np.array(data[data_columns]) @ eigen_vec\n","    return Y, eigen_vec\n","\n","def proximity_test(X, y, X_test, sampling_rate=8, dsfn=lambda x,y: sum((x-y)**2)):\n","    \"\"\"\n","    Return: list of label prediction according to X_test with sampling rate\n","    \"\"\"\n","\n","    assert sampling_rate > 0, \"Sampling rate must be greater than 0 !!\"\n","\n","    N_av = sampling_rate\n","\n","    labels = np.unique(y)\n","    N_l = {label: sum(y==label) for label in labels}\n","    pred = []\n","\n","    score_geo = {}\n","    for label in labels:\n","        score = 0.\n","        for xli in X[y == label]:\n","            for xlj in X[y == label]:\n","                score += dsfn(xli, xlj)\n","        score /= 2 * N_l[label] * N_l[label]\n","        score_geo[label] = score\n","\n","    for t in range(len(X_test) // sampling_rate):\n","        best = (None, None) # label, score\n","        for label in labels:\n","            score = 0.\n","            for xt in X_test[t*sampling_rate:(t+1)*sampling_rate]:\n","                score_rel = 0.\n","                for xlj in X[y == label]:\n","                    score_rel += dsfn(xt, xlj)\n","                score_rel /= N_l[label]\n","                score += score_rel - score_geo[label]\n","            score /= N_av\n","            if best[1] is None or best[1] > score:\n","                best = (label, score)\n","        pred.append(best)\n","\n","    return np.array([ t[0] for t in pred ])\n","\n","def compute_interest_df_index(df, train_input_index, test_input_index):\n","\n","    train_index = pd.Series([False] * df.shape[0])\n","    test_index = pd.Series([False] * df.shape[0])\n","\n","    for subject, session in train_input_index:\n","        train_index |= (df['subject'] == subject) & (df['session'] == session)\n","    \n","    for subject, session in test_input_index:\n","        test_index |= (df['subject'] == subject) & (df['session'] == session)\n","        \n","    all_index = train_index | test_index\n","    \n","    return (train_index, test_index, all_index)\n","\n","def dbda(df, train_input_index, test_input_index, pcf_channels, preprocess='cvt', sampling_rate=8):\n","    \"\"\"\n","    train_input_index: [(subject id, session id), ...]\n","    test_input_index: [(subject id, session id), ...]\n","    \"\"\"\n","\n","    data = df.copy()\n","    data_columns = data.drop(columns=['label', 'session', 'subject']).columns\n","    \n","    train_index, test_index, all_index = compute_interest_df_index(df, train_input_index, test_input_index)\n","    \n","    if preprocess == 'cvt':\n","        # Normalize\n","        # data: dataframe\n","        data = df_channel_norm(data, pcf_channels)\n","\n","        # CVT\n","        # Y, eigen_vec: numpy.arrray\n","        Y, eigen_vec = cvt(data[train_index])\n","\n","        # Construct variable\n","        # All variables below are numpy.array\n","        X_train = Y\n","\n","        X_val = np.array(data[test_index][data_columns]) @ eigen_vec\n","\n","    elif preprocess == 'pca':\n","        # Normalize\n","        # data: dataframe\n","        data = df_channel_standardize(data, pcf_channels)\n","\n","        pca = PCA(n_components=50)\n","        pca.fit(data[train_index][data_columns])\n","        X_train = pca.transform(data[train_index][data_columns])\n","        X_val = pca.transform(data[test_index][data_columns])\n","\n","    else:\n","        raise Exception('Mismatched preprocess: either cvt or pca')\n","\n","    y_train = data[train_index]['label'].values\n","    y_val = data[test_index]['label'].values[::-sampling_rate][::-1] # label from last timestep each sample\n","\n","    # Proxity test (DBDA)\n","    y_pred = proximity_test(X_train, y_train, X_val, sampling_rate=sampling_rate)\n","\n","    # Print accuracy\n","    accuracy = accuracy_score(y_val, y_pred)\n","    print('Train: {}, Test: {}, Accuracy: {:.2f}'.format(train_input_index, test_input_index, accuracy*100))\n","    \n","    return accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2aEBjCLR0rJY","colab_type":"code","outputId":"4381d0cb-b4cd-4961-9482-d672b3956c46","executionInfo":{"status":"ok","timestamp":1554430408948,"user_tz":-60,"elapsed":2049936,"user":{"displayName":"Danuphan Suwanwong","photoUrl":"","userId":"16514894279538784287"}},"colab":{"base_uri":"https://localhost:8080/","height":629}},"source":["test_condition = [\n","    ([(1,1), (1,2)], [(1,3)]),\n","    ([(2,1), (2,2)], [(2,3)]),\n","    ([(3,1), (3,2)], [(3,3)]),\n","    ([(1,1)], [(1,2)]),\n","    ([(1,1)], [(1,3)]),\n","    ([(1,2)], [(1,3)]),\n","    ([(2,1)], [(2,2)]),\n","    ([(2,1)], [(2,3)]),\n","    ([(2,2)], [(2,3)]),\n","    ([(3,1)], [(3,2)]),\n","    ([(3,1)], [(3,3)]),\n","    ([(3,2)], [(3,3)]),\n","]\n","\n","results_cvt = []\n","\n","for train_con, test_con in test_condition:\n","    %time acc = dbda(df_train_pcf, train_con, test_con, pcf_channels, preprocess='cvt')\n","    results_cvt.append(acc)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train: [(1, 1), (1, 2)], Test: [(1, 3)], Accuracy: 73.09\n","CPU times: user 5min 22s, sys: 510 ms, total: 5min 22s\n","Wall time: 5min 22s\n","Train: [(2, 1), (2, 2)], Test: [(2, 3)], Accuracy: 58.99\n","CPU times: user 5min 7s, sys: 590 ms, total: 5min 7s\n","Wall time: 5min 7s\n","Train: [(3, 1), (3, 2)], Test: [(3, 3)], Accuracy: 41.40\n","CPU times: user 5min 4s, sys: 793 ms, total: 5min 5s\n","Wall time: 5min 5s\n","Train: [(1, 1)], Test: [(1, 2)], Accuracy: 70.74\n","CPU times: user 1min 58s, sys: 265 ms, total: 1min 58s\n","Wall time: 1min 58s\n","Train: [(1, 1)], Test: [(1, 3)], Accuracy: 72.42\n","CPU times: user 2min, sys: 279 ms, total: 2min\n","Wall time: 2min\n","Train: [(1, 2)], Test: [(1, 3)], Accuracy: 75.34\n","CPU times: user 2min 11s, sys: 283 ms, total: 2min 11s\n","Wall time: 2min 11s\n","Train: [(2, 1)], Test: [(2, 2)], Accuracy: 51.85\n","CPU times: user 2min 5s, sys: 296 ms, total: 2min 5s\n","Wall time: 2min 5s\n","Train: [(2, 1)], Test: [(2, 3)], Accuracy: 54.84\n","CPU times: user 2min 6s, sys: 282 ms, total: 2min 6s\n","Wall time: 2min 6s\n","Train: [(2, 2)], Test: [(2, 3)], Accuracy: 58.76\n","CPU times: user 2min 7s, sys: 318 ms, total: 2min 7s\n","Wall time: 2min 7s\n","Train: [(3, 1)], Test: [(3, 2)], Accuracy: 53.04\n","CPU times: user 1min 59s, sys: 285 ms, total: 1min 59s\n","Wall time: 1min 59s\n","Train: [(3, 1)], Test: [(3, 3)], Accuracy: 46.28\n","CPU times: user 2min, sys: 342 ms, total: 2min\n","Wall time: 2min\n","Train: [(3, 2)], Test: [(3, 3)], Accuracy: 42.79\n","CPU times: user 2min 3s, sys: 301 ms, total: 2min 4s\n","Wall time: 2min 4s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vkKKc_IoTQEu","colab_type":"code","outputId":"7d89f92b-96b4-41a8-e619-22fc800bdf44","executionInfo":{"status":"ok","timestamp":1554438835180,"user_tz":-60,"elapsed":2093147,"user":{"displayName":"Danuphan Suwanwong","photoUrl":"","userId":"16514894279538784287"}},"colab":{"base_uri":"https://localhost:8080/","height":629}},"source":["test_condition = [\n","    ([(1,1), (1,2)], [(1,3)]),\n","    ([(2,1), (2,2)], [(2,3)]),\n","    ([(3,1), (3,2)], [(3,3)]),\n","    ([(1,1)], [(1,2)]),\n","    ([(1,1)], [(1,3)]),\n","    ([(1,2)], [(1,3)]),\n","    ([(2,1)], [(2,2)]),\n","    ([(2,1)], [(2,3)]),\n","    ([(2,2)], [(2,3)]),\n","    ([(3,1)], [(3,2)]),\n","    ([(3,1)], [(3,3)]),\n","    ([(3,2)], [(3,3)]),\n","]\n","\n","results_pca = []\n","\n","for train_con, test_con in test_condition:\n","    %time acc = dbda(df_train_pcf, train_con, test_con, pcf_channels, preprocess='pca')\n","    results_pca.append(acc)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train: [(1, 1), (1, 2)], Test: [(1, 3)], Accuracy: 72.42\n","CPU times: user 5min 17s, sys: 735 ms, total: 5min 17s\n","Wall time: 5min 17s\n","Train: [(2, 1), (2, 2)], Test: [(2, 3)], Accuracy: 58.29\n","CPU times: user 5min 42s, sys: 866 ms, total: 5min 43s\n","Wall time: 5min 45s\n","Train: [(3, 1), (3, 2)], Test: [(3, 3)], Accuracy: 42.79\n","CPU times: user 5min 7s, sys: 801 ms, total: 5min 8s\n","Wall time: 5min 9s\n","Train: [(1, 1)], Test: [(1, 2)], Accuracy: 66.13\n","CPU times: user 2min 6s, sys: 392 ms, total: 2min 6s\n","Wall time: 2min 6s\n","Train: [(1, 1)], Test: [(1, 3)], Accuracy: 69.51\n","CPU times: user 2min 7s, sys: 525 ms, total: 2min 8s\n","Wall time: 2min 8s\n","Train: [(1, 2)], Test: [(1, 3)], Accuracy: 75.11\n","CPU times: user 2min 6s, sys: 450 ms, total: 2min 7s\n","Wall time: 2min 6s\n","Train: [(2, 1)], Test: [(2, 2)], Accuracy: 51.16\n","CPU times: user 2min 4s, sys: 471 ms, total: 2min 5s\n","Wall time: 2min 5s\n","Train: [(2, 1)], Test: [(2, 3)], Accuracy: 54.38\n","CPU times: user 2min 5s, sys: 554 ms, total: 2min 6s\n","Wall time: 2min 6s\n","Train: [(2, 2)], Test: [(2, 3)], Accuracy: 52.53\n","CPU times: user 2min 3s, sys: 524 ms, total: 2min 4s\n","Wall time: 2min 3s\n","Train: [(3, 1)], Test: [(3, 2)], Accuracy: 54.91\n","CPU times: user 2min 1s, sys: 448 ms, total: 2min 1s\n","Wall time: 2min 1s\n","Train: [(3, 1)], Test: [(3, 3)], Accuracy: 41.86\n","CPU times: user 2min, sys: 399 ms, total: 2min 1s\n","Wall time: 2min 1s\n","Train: [(3, 2)], Test: [(3, 3)], Accuracy: 40.47\n","CPU times: user 2min, sys: 351 ms, total: 2min\n","Wall time: 2min\n"],"name":"stdout"}]}]}