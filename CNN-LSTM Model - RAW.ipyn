{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BOSS - CNN-LSTM Model - RAW.ipyn","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6GEZ0gtRXWoZ","colab_type":"text"},"source":["ref: https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/"]},{"cell_type":"code","metadata":{"id":"6Je6OUkqqsif","colab_type":"code","colab":{}},"source":["import os\n","from scipy.io import loadmat\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","import pickle\n","import re\n","\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9wT0eWLNqsii","colab_type":"text"},"source":["## Read Data"]},{"cell_type":"markdown","metadata":{"id":"VKUn9bgNj7Gl","colab_type":"text"},"source":["### Mount google drive"]},{"cell_type":"code","metadata":{"id":"buThuWRhjOvM","colab_type":"code","colab":{}},"source":["# Mount Google drive\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdkRlyuMjqoh","colab_type":"code","colab":{}},"source":["# Here you must try by yourself where the path to the dataset folder is !!\n","# e.g. !ls \"gdrive/My Drive/Colab Notebooks/DME/datasets\"\n","!ls \"gdrive\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YSu2jDSckK5B","colab_type":"code","colab":{}},"source":["# Then assign the path here.\n","# This is the path for my GG drive, change it according to your path\n","# dataset_path = \"gdrive/My Drive/Colab Notebooks/DME/datasets\"\n","\n","# Your folder path here\n","dataset_path = \"gdrive/My Drive/Colab Notebooks/DME/datasets\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FqSsDNBpqsip","colab_type":"text"},"source":["### Read data from csv"]},{"cell_type":"code","metadata":{"id":"PV64-5xDqsip","colab_type":"code","colab":{}},"source":["try:\n","  dataset_path\n","except:\n","  dataset_path = os.path.join(os.getcwd(), 'datasets')\n","\n","df_train_pcf = pd.read_csv(os.path.join(dataset_path, 'df_train_pcf.csv'))\n","df_test_pcf = pd.read_csv(os.path.join(dataset_path, 'df_test_pcf.csv'))\n","df_train_raw = pd.read_csv(os.path.join(dataset_path, 'df_train_raw.csv'))\n","df_test_raw = pd.read_csv(os.path.join(dataset_path, 'df_test_raw.csv'))\n","\n","label_dict = {2: 'left', 3: 'right', 7: 'word'}\n","df_train_pcf.replace({'label' : label_dict}, inplace=True)\n","df_train_raw.replace({'label' : label_dict}, inplace=True)\n","\n","pcf_setup = pickle.load(open(os.path.join(dataset_path, 'pcf_setup.obj'), \"rb\"))\n","raw_setup = pickle.load(open(os.path.join(dataset_path, 'raw_setup.obj'), \"rb\"))\n","\n","pcf_channels = pcf_setup['channels']\n","raw_channels = raw_setup['channels']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rJcl1hcL4gkM","colab_type":"text"},"source":["## CNN-LSTM"]},{"cell_type":"code","metadata":{"id":"TjzpoT0JlT2x","colab_type":"code","outputId":"0524a18b-d65a-453a-caea-187cf8ab2b64","executionInfo":{"status":"ok","timestamp":1554297148335,"user_tz":-60,"elapsed":17579,"user":{"displayName":"Danuphan Suwanwong","photoUrl":"","userId":"16514894279538784287"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["df_train_pcf.shape, df_train_raw.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((31216, 99), (1096192, 35))"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"ETpqnjuf6dDu","colab_type":"code","outputId":"b6ad2902-05ed-4825-b218-88f9e86870c3","executionInfo":{"status":"ok","timestamp":1554297149747,"user_tz":-60,"elapsed":18351,"user":{"displayName":"Danuphan Suwanwong","photoUrl":"","userId":"16514894279538784287"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.layers import TimeDistributed\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","from keras.utils import to_categorical"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"9tjKa7cO4qoM","colab_type":"code","colab":{}},"source":["# Load dataset\n","def load_dataset(df, sessions=[1,2,3], subjects=[1,2,3], norm=False):\n","    data = df.copy()\n","    sampling_rate = 512\n","\n","    interest_index = data['session'].isin(sessions) & data['subject'].isin(subjects)\n","\n","    data_columns = data.drop(columns=['label', 'session', 'subject']).columns\n","  \n","    label_dict = {'left': 0, 'right': 1, 'word': 2}\n","    data.replace({'label' : label_dict}, inplace=True)\n","\n","    X = data[interest_index][data_columns].values\n","    if norm:\n","        X = (X - X.mean(axis=0)) / X.std(axis=0) # normailise\n","    y = data[interest_index]['label'].values\n","  \n","    n_features = X.shape[1]\n","  \n","    X = X.reshape(X.shape[0]//sampling_rate, sampling_rate, n_features)\n","    y = to_categorical(y[::sampling_rate])\n","\n","    trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.3, random_state=1)\n","    return trainX, trainy, testX, testy\n","\n","def load_dataset_by_subjects(df, subjects_train=[1,2], subjects_test=[3], norm=False):\n","    data = df.copy()\n","    sampling_rate = 512\n","\n","    train_index = data['subject'].isin(subjects_train)\n","    test_index = data['subject'].isin(subjects_test)\n","\n","    data_columns = data.drop(columns=['label', 'session', 'subject']).columns\n","  \n","    label_dict = {'left': 0, 'right': 1, 'word': 2}\n","    data.replace({'label' : label_dict}, inplace=True)\n","\n","    trainX = data[train_index][data_columns].values\n","    testX = data[test_index][data_columns].values\n","    trainy = data[train_index]['label'].values\n","    testy = data[test_index]['label'].values\n","  \n","    n_features = trainX.shape[1]\n","\n","    trainX = trainX.reshape(trainX.shape[0]//sampling_rate, sampling_rate, n_features)\n","    testX = testX.reshape(testX.shape[0]//sampling_rate, sampling_rate, n_features)\n","    trainy = to_categorical(trainy[::sampling_rate])\n","    testy = to_categorical(testy[::sampling_rate])\n","\n","    return trainX, trainy, testX, testy\n","\n","# fit and evaluate a model\n","def evaluate_model(trainX, trainy, testX, testy):\n","    # define model\n","    verbose, epochs, batch_size = 0, 25, 16\n","    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n","    # reshape data into time steps of sub-sequences\n","    n_steps, n_length = 16, 32\n","    trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n","    testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n","    # define model\n","    model = Sequential()\n","    model.add(TimeDistributed(Conv1D(filters=128, kernel_size=9, activation='relu'), input_shape=(None,n_length,n_features)))\n","    model.add(TimeDistributed(Conv1D(filters=128, kernel_size=9, activation='relu')))\n","    model.add(TimeDistributed(Dropout(0.5)))\n","    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n","    model.add(TimeDistributed(Flatten()))\n","    model.add(LSTM(100))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(100, activation='relu'))\n","    model.add(Dense(n_outputs, activation='softmax'))\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    # fit network\n","#   model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n","    model.fit(trainX, trainy, epochs=epochs, verbose=verbose)\n","    # evaluate model\n","#   _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n","    _, accuracy = model.evaluate(testX, testy, verbose=0)\n","    return accuracy\n","\n","# summarize scores\n","def summarize_results(scores):\n","    print(scores)\n","    m, s = np.mean(scores), np.std(scores)\n","    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n","\n","# Normalize features per channel\n","def df_channel_norm(df, channels):\n","    df_norm = df.copy()\n","    for channel in pcf_channels:\n","        df_channel = df_norm.filter(regex=channel)\n","        df_norm[df_channel.columns] = df_channel.div(df_channel.sum(axis=1), axis='index')\n","    return df_norm\n","\n","# run an experiment\n","def run_experiment(df, repeats=10, sessions=[1,2,3], subjects=[1,2,3]):\n","    # load data\n","    trainX, trainy, testX, testy = load_dataset(df, sessions, subjects)\n","    # repeat experiment\n","    scores = list()\n","    for r in range(repeats):\n","        score = evaluate_model(trainX, trainy, testX, testy)\n","        score = score * 100.0\n","        print('>#%d: %.3f' % (r+1, score))\n","        scores.append(score)\n","    # summarize results\n","    summarize_results(scores)\n","\n","def run_experiment_by_subjects(df, repeats=10, subjects_train=[1,2], subjects_test=[3]):\n","    # load data\n","    trainX, trainy, testX, testy = load_dataset_by_subjects(df, subjects_train, subjects_test)\n","    # repeat experiment\n","    scores = list()\n","    for r in range(repeats):\n","        score = evaluate_model(trainX, trainy, testX, testy)\n","        score = score * 100.0\n","        print('>#%d: %.3f' % (r+1, score))\n","        scores.append(score)\n","    # summarize results\n","    summarize_results(scores)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PV98J1PTGkXc","colab_type":"code","outputId":"fbf55c16-cfb6-43c4-fa50-6854b681033f","executionInfo":{"status":"ok","timestamp":1554297188902,"user_tz":-60,"elapsed":5062,"user":{"displayName":"Danuphan Suwanwong","photoUrl":"","userId":"16514894279538784287"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["trainX, trainy, testX, testy = load_dataset(df_train_raw)\n","trainX.shape, trainy.shape, testX.shape, testy.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1498, 512, 32), (1498, 3), (643, 512, 32), (643, 3))"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"qlsXd2fJ2dkw","colab_type":"code","outputId":"d5a71b9a-24cb-4af5-a5ca-633142e3240a","executionInfo":{"status":"ok","timestamp":1554297474371,"user_tz":-60,"elapsed":254767,"user":{"displayName":"Danuphan Suwanwong","photoUrl":"","userId":"16514894279538784287"}},"colab":{"base_uri":"https://localhost:8080/","height":275}},"source":["run_experiment_by_subjects(df_train_raw, subjects_train=[1,2], subjects_test=[3], repeats=3)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n",">#1: 33.474\n",">#2: 33.474\n",">#3: 33.474\n","[33.47398030942335, 33.47398030942335, 33.47398030942335]\n","Accuracy: 33.474% (+/-0.000)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cOvht2o0BBro","colab_type":"code","outputId":"94e27f54-2489-402d-e7da-5fb002d80a03","executionInfo":{"status":"ok","timestamp":1554261726146,"user_tz":-60,"elapsed":1587698,"user":{"displayName":"Danuphan Suwanwong","photoUrl":"","userId":"16514894279538784287"}},"colab":{"base_uri":"https://localhost:8080/","height":241}},"source":["# run the experiment\n","run_experiment(df_train_raw, sessions=[1,2,3], subjects=[1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":[">#1: 38.605\n",">#2: 38.605\n",">#3: 38.605\n",">#4: 30.698\n",">#5: 36.279\n",">#6: 38.605\n",">#7: 38.605\n",">#8: 38.605\n",">#9: 38.605\n",">#10: 38.605\n","[38.60465112120606, 38.60465112120606, 38.60465112120606, 30.69767448791238, 36.27906961496486, 38.60465112120606, 38.60465112120606, 38.60465112120606, 38.60465112120606, 38.60465112120606]\n","Accuracy: 37.581% (+/-2.397)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pQ8iTOlVJ7xg","colab_type":"code","outputId":"fa64f578-4137-4e4e-b9b9-348289be25ab","executionInfo":{"status":"ok","timestamp":1554262296396,"user_tz":-60,"elapsed":2157947,"user":{"displayName":"Danuphan Suwanwong","photoUrl":"","userId":"16514894279538784287"}},"colab":{"base_uri":"https://localhost:8080/","height":241}},"source":["# run the experiment\n","run_experiment(df_train_raw, sessions=[1,2,3], subjects=[2])"],"execution_count":0,"outputs":[{"output_type":"stream","text":[">#1: 37.209\n",">#2: 37.209\n",">#3: 36.279\n",">#4: 37.209\n",">#5: 37.209\n",">#6: 37.209\n",">#7: 37.209\n",">#8: 37.209\n",">#9: 37.209\n",">#10: 37.209\n","[37.20930238102758, 37.20930238102758, 36.27906972585723, 37.20930238102758, 37.20930238102758, 37.20930238102758, 37.20930238102758, 37.20930238102758, 37.20930238102758, 37.20930238102758]\n","Accuracy: 37.116% (+/-0.279)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GJXpRknvJ7zv","colab_type":"code","outputId":"c194da98-9342-4255-b262-ba4fa3bfd1c3","colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["# run the experiment\n","run_experiment(df_train_raw, sessions=[1,2,3], subjects=[3])"],"execution_count":0,"outputs":[{"output_type":"stream","text":[">#1: 35.514\n",">#2: 34.579\n",">#3: 36.449\n",">#4: 34.579\n",">#5: 31.308\n",">#6: 34.112\n",">#7: 34.579\n",">#8: 34.579\n",">#9: 34.579\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BIjih8N5J9sc","colab_type":"code","colab":{}},"source":["# run the experiment\n","run_experiment(df_train_raw, sessions=[1,2,3], subjects=[1,2,3])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IzrIBY0VrK9M","colab_type":"code","colab":{}},"source":["# run the experiment\n","run_experiment(df_train_raw, sessions=[1,2,3], subjects=[1,2])"],"execution_count":0,"outputs":[]}]}